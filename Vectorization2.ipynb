{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b77d5e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Science in TSE Systems/MISP Project\n",
    "#Code written by: Anshak Mallik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa46fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f08d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the dataframe from Attributes.csv\n",
    "data_folder = \"Data\"\n",
    "attributes = pd.read_csv(f\"{data_folder}/Attributes.csv\")  #dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6b0199",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "161690bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "value = attributes['Value']\n",
    "attribute_type = attributes['Attribute Type']\n",
    "\n",
    "def normalize(tokens):\n",
    "    '''\n",
    "    INPUT:  - tokens: list of tokens represented as strings\n",
    "    OUTPUT: - normalized_tokens: list of input tokens which have been normalized\n",
    "    '''\n",
    "    \n",
    "    normalized_tokens = []\n",
    "    \n",
    "    for token, i in zip(tokens, range(len(tokens))):\n",
    "        \n",
    "        #Removing punctuation\n",
    "        if attribute_type[i] != ('ip-dst' or 'ip-dst|port' or 'ip-src' or 'ip-src|port'):\n",
    "            \n",
    "            #Removing 'http[s]://' and 'www' from beginning of urls\n",
    "            if (token.startswith('http')):\n",
    "                token = re.sub('http[s]?://','', token)\n",
    "            if (token.startswith('www')):\n",
    "                token = re.sub('www', '', token)\n",
    "                \n",
    "            #Remove punctuation\n",
    "            token = re.sub(r'[^\\w\\s]', '', token)\n",
    "        \n",
    "        #Making all tokens lower case\n",
    "        token = token.lower()\n",
    "        \n",
    "        #Appending to list\n",
    "        normalized_tokens.append(token)\n",
    "    \n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d24c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing values to normalised values in dataframe\n",
    "normalised_values = normalize(value)\n",
    "attributes['Value'] = pd.Series(normalised_values)\n",
    "value = attributes['Value'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "461ffd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "#Creating list with all event IDs (without repetition)\n",
    "event_ids = attributes['Event ID']\n",
    "events = list(OrderedDict.fromkeys(event_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf3e9c8",
   "metadata": {},
   "source": [
    "## Inheritance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b2f5bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following event has too many values (more than 5000)!\n",
      "Index 218, Event ID 79819, Values 29641\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 335, Event ID 79837, Values 30752\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 602, Event ID 62222, Values 41630\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 2534, Event ID 86173, Values 7814\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 2721, Event ID 14027, Values 33457\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 2939, Event ID 14168, Values 41542\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 3045, Event ID 14223, Values 40520\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 3394, Event ID 62248, Values 34623\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 4333, Event ID 6271, Values 5832\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 4595, Event ID 14320, Values 6880\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 4744, Event ID 69573, Values 11063\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 5033, Event ID 14741, Values 45529\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 5148, Event ID 86170, Values 5244\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 5272, Event ID 15538, Values 19471\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 5313, Event ID 86281, Values 8903\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 5440, Event ID 14711, Values 48083\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 6133, Event ID 14222, Values 41165\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 6144, Event ID 14449, Values 47654\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 6322, Event ID 26321, Values 38790\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 6436, Event ID 90141, Values 8168\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 6591, Event ID 97697, Values 11264\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 6700, Event ID 44677, Values 53184\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 6878, Event ID 62336, Values 21910\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 7138, Event ID 14388, Values 36887\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 7947, Event ID 42970, Values 58354\n",
      "----------\n",
      "The following event has too many values (more than 5000)!\n",
      "Index 8223, Event ID 14265, Values 17340\n",
      "----------\n",
      "Total number of events with values over 5000: 26\n"
     ]
    }
   ],
   "source": [
    "#Loop to see which events have number of values of tolerance\n",
    "tol = 5e3\n",
    "counter = 0\n",
    "big_events = []\n",
    "big_indices = []\n",
    "\n",
    "for k in range(len(events)):\n",
    "    #Getting relevant rows and their attribute IDs\n",
    "    event_df = attributes.loc[attributes['Event ID'] == events[k]]\n",
    "    n = len(event_df)\n",
    "    attribute_ids = event_df['Attribute ID']\n",
    "    values = np.array(event_df['Value'])\n",
    "    if len(values) >= tol: \n",
    "        print('The following event has too many values (more than %d)!' % (tol))\n",
    "        print('Index %d, Event ID %d, Values %d' % (k, events[k], len(values)))\n",
    "        print('----------')\n",
    "        counter += 1\n",
    "        big_events.append(events[k])\n",
    "        big_indices.append(k)\n",
    "        \n",
    "print('Total number of events with values over %d: %d' % (tol, counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29f0a4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering events before getting attribute IDs\n",
    "filtered_events = [x for x in events if x not in big_events]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b82b92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from timeit import default_timer as timer\n",
    "# from datetime import timedelta\n",
    "# from difflib import SequenceMatcher\n",
    "# # import jellyfish as jl\n",
    "\n",
    "# #Creating subset matrix for example\n",
    "\n",
    "# #Begin timer before loop\n",
    "# start = timer()\n",
    "\n",
    "# #Getting subset data\n",
    "# N = 100\n",
    "# attribute_ids_N = np.array(attributes['Attribute ID'][:N]) #Series\n",
    "# values_N = value[:N] #After normalization\n",
    "\n",
    "# #Empty matrix\n",
    "# I = np.zeros((N, N))\n",
    "# for i in range(N):\n",
    "#     for j in range(N):\n",
    "#         if (i >= j):\n",
    "#             I[i,j] = round(SequenceMatcher(None, values_N[i], values_N[j]).ratio(), 3)\n",
    "#             # I[i,j] = round(jl.jaro_similarity(values_N[i], values_N[j]), 3)\n",
    "            \n",
    "# #Matrix as dataframe\n",
    "# df = pd.DataFrame(I, index=attribute_ids_N, columns=attribute_ids_N)\n",
    "# # df.to_csv(r'Data/Inheritance_subset.csv')\n",
    "\n",
    "# #End timer after loop    \n",
    "# end = timer()\n",
    "# print(timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3bcc128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 iterations done in: \n",
      "0:00:00.003724\n",
      "----------\n",
      "1000 iterations done in: \n",
      "0:00:02.614534\n",
      "----------\n",
      "2000 iterations done in: \n",
      "0:00:05.670118\n",
      "----------\n",
      "3000 iterations done in: \n",
      "0:00:07.702613\n",
      "----------\n",
      "4000 iterations done in: \n",
      "0:00:11.083422\n",
      "----------\n",
      "5000 iterations done in: \n",
      "0:00:13.636675\n",
      "----------\n",
      "6000 iterations done in: \n",
      "0:00:16.747232\n",
      "----------\n",
      "7000 iterations done in: \n",
      "0:00:19.084246\n",
      "----------\n",
      "8000 iterations done in: \n",
      "0:00:22.750406\n",
      "----------\n",
      "0:00:23.320976\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "#Getting list of all event and attribute IDs separately\n",
    "start = timer()\n",
    "e, a, index = [], [], []\n",
    "for k in range(len(filtered_events)):\n",
    "    \n",
    "    #Filtering using events\n",
    "    df = attributes.loc[attributes['Event ID'] == filtered_events[k]]\n",
    "    #Attribute ID\n",
    "    attribute_ids = df['Attribute ID']\n",
    "    \n",
    "    #Indices of rows\n",
    "    index_list = list(df.index)\n",
    "    index += index_list\n",
    "    #Appending e and a\n",
    "    for l in range(len(attribute_ids)):\n",
    "        e.append(filtered_events[k])\n",
    "        a.append(np.array(attribute_ids)[l])\n",
    "        \n",
    "    #Checkpoints\n",
    "    if k%1000 == 0:\n",
    "        print('%d iterations done in: ' % (k))\n",
    "        print(timedelta(seconds=timer()-start))\n",
    "        print('----------')\n",
    "print(timedelta(seconds=timer()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c49706b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask\n",
      "  Downloading dask-2022.5.0-py3-none-any.whl (1.1 MB)\n",
      "Requirement already satisfied: cloudpickle>=1.1.1 in c:\\users\\ansha\\anaconda3\\lib\\site-packages (from dask) (2.0.0)\n",
      "Collecting partd>=0.3.10\n",
      "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
      "Collecting fsspec>=0.6.0\n",
      "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
      "Collecting toolz>=0.8.2\n",
      "  Downloading toolz-0.11.2-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ansha\\anaconda3\\lib\\site-packages (from dask) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\ansha\\anaconda3\\lib\\site-packages (from dask) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ansha\\anaconda3\\lib\\site-packages (from packaging>=20.0->dask) (3.0.4)\n",
      "Collecting locket\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: toolz, locket, partd, fsspec, dask\n",
      "Successfully installed dask-2022.5.0 fsspec-2022.3.0 locket-1.0.0 partd-1.2.0 toolz-0.11.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a909f3ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 683. GiB for an array with shape (302875, 302875) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m start \u001b[38;5;241m=\u001b[39m timer()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#Empty matrix\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m I \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 683. GiB for an array with shape (302875, 302875) and data type float64"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "#Creating matrix\n",
    "\n",
    "#Getting the data\n",
    "N = len(index)\n",
    "labels = [(ev, at) for ev, at in zip(e, a)]\n",
    "values = value[index]\n",
    "\n",
    "#Begin timer before loop\n",
    "start = timer()\n",
    "\n",
    "#Empty matrix\n",
    "I = np.zeros((N, N))    \n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        if (i >= j):\n",
    "            I[i,j] = round(SequenceMatcher(None, values[i], values[j]).ratio(), 3)\n",
    "    #Checkpoints\n",
    "    if i%1000 == 0:\n",
    "        print('%d iterations done in: ' % (k))\n",
    "        print(timedelta(seconds=timer()-start))\n",
    "        print('----------')\n",
    "        \n",
    "#Matrix as dataframe\n",
    "print('-------------------- \\n%Saving to .csv%')\n",
    "df = pd.DataFrame(I, index=labels, columns=labels)\n",
    "df.to_csv(r'Data/Inheritance.csv')\n",
    "\n",
    "#End timer after loop    \n",
    "end = timer()\n",
    "print(timedelta(seconds=end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf85db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
